{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we may be interested in the distribution of a transformation of a random variable. For example, if we know the distribution of $X$, we may wish to know the distribution of $X^2$ or $2X$. \n",
    "\n",
    "It helps to consider the pmf/cdf of the original random variables. Let $Y=t(X)$ where $X$ is discrete:\n",
    "\n",
    "$$\n",
    "f_Y(y)=P(Y=y) = P(t(X)=y) = P( X = t^{-1}(y))\n",
    "$$\n",
    "\n",
    "In the continuous case, let's consider the cdf:\n",
    "\n",
    "$$\n",
    "F_Y(y)=P(Y\\leq y) = P(t(X)\\leq y) = P(X \\leq t^{-1}(y)) = F_X(t^{-1}(y))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1\n",
    "\n",
    "Suppose the pmf for $X$ is given by the following table: \n",
    "\n",
    " | value of $X$  | -2 | -1 | 0 | 1 | 2 | \n",
    " | ------ | ------ | ----- | ----- | ----- | ----- |\n",
    " | probability | 0.05 | 0.10 | 0.35 | 0.30 | 0.20 |\n",
    "\n",
    "Find the distribution of $X^2$ and calculate $E(X^2)$. Does $E(X^2) = [E(X)]^2$? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The E(X^2) is 1.4 and E(X)^2 is 0.25 . These results are not equal.\n"
     ]
    }
   ],
   "source": [
    "# Distribution of X^2 based upon mapping\n",
    "P0 = 0.35\n",
    "P1 = 0.30 + 0.10\n",
    "P4 = 0.05 + 0.20\n",
    "EX_squared = 0*P0 + 1*P1 + 4*P4 # using E(x) = sum(x*f(x))\n",
    "EX = -2*0.05 + -1*0.10 + 0*0.35 + 1*0.30 + 2*0.20 # manually entering values instead of collecting row values, as above\n",
    "print('The E(X^2) is',EX_squared,'and E(X)^2 is',EX ** 2,'. These results are not equal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2\n",
    "Let $X \\sim \\textsf{Binom}(n,p)$. What is the pmf for $X+3$? Make sure you specify the domain of $X+3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X ~ Binom(n,p), then the pmf for X equals $${n \\choose x}*p^x*(1-p)^{n-x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Y = X+3, we get $$t^{-1}(y) = X$$ $$t^{-1}(y) = Y-3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the full equality above, we have $$F_y(y) = P(Y\\leq y) = P(X+3\\leq y) = P(X\\leq Y-3) = F_x(Y-3)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the pmf for X+3 is equal to $$F_y(y) = {n \\choose y-3}*p^{y-3}*(1-p)^{n-y+3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the domain has been shifted from [1,n] by 3, we find that the domain of X+3 is [3,n+3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3\n",
    "\n",
    "Let $X \\sim \\textsf{Unif}(0,1)$. Let $Y=X^2$. Find the **pdf** of $Y$. Again, specify the domain of $Y$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If X ~ Unif(0,1), then $f_x(X) = 1$ and $$F_x(X) = \\int\\limits_0^x f_x(x) dx$$ Letting Y = $X^2$, we know that $$f_y(Y) = \\frac{d}{dx}*F_y(Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find $F_y(Y)$, we use the cdf operations above. For expediency, we get $t^{-1}(y) = \\sqrt{y}$ and since we have a uniform distribution, $F_y(Y) = \\sqrt{x}$ as well. Taking the derivative, this means that the pdf of Y is $$f_y(Y) = \\frac{0.5}{\\sqrt{y}}$$ where y is an element of the doman (0,1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment Generating Functions (MGF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One powerful concept in probability is the moment generating function (mgf). Let $X$ be a random variable. The mgf of $X$ is denoted by $M_X(t)$. This function is powerful because it can be used as a shortcut to find the $k$th central moment. Specifically,\n",
    "\n",
    "$$\n",
    "E(X^k) = \\frac{d^k}{dt^k} M_X(t) \\bigg |_{t=0}\n",
    "$$\n",
    "\n",
    "If you know the moment generating function of $X$, you can simply take the derivative of it with respect to $t$, evaluate at $t=0$ and the result is the expected value of $X$, $E(X)$. \n",
    "\n",
    "The mgf of $X$ is found by\n",
    "\n",
    "$$\n",
    "M_X(t) = E(e^{tX})\n",
    "$$\n",
    "\n",
    "#### Example 4: \n",
    "\n",
    "Let $X$ be a random variable with the exponential distribution with parameter $\\lambda >0$. Recall that $f_X(x) = \\lambda e^{-\\lambda x}$, for $x>0$. Find the mgf of $X$. Use it to verify that $E(X) = \\frac{1}{\\lambda}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have that $$E(X^k) = \\frac{d^k}{dt^k}M_x(t)|_{t=0}$$ Therefore, the mgf of X is $$M_x(t) = \\int \\limits_x e^{tX}$$ $$M_x(t) = \\int \\limits_x e^{tX}\\lambda e^{-\\lambda x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this integral to converge, when evaluated from 0 to $\\infty$, t must be less than $\\lambda$. Solving, we get $$M_x(t) = -\\frac{\\lambda}{t-\\lambda}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the expected value, we take the first derivative of $M_x(t)$ with respect to t evaluated at $t = 0$ and get $E(x) = \\frac{1}{\\lambda}$. Here, $\\lambda$ must be greater than 0 for the function to be defined at $t = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 5:\n",
    "\n",
    "The moment generating function of a random variable with the binomial distribution (with parameters $n$ and $p$) is given by $M_X(t) = (pe^t + 1 - p)^n$. Use the mgf to verify that $E(X)=np$ and $V(X)=np(1-p)$. Note that $V(X)=E(X^2)-[E(X)]^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we evaluate the first time derivative of $M_x(t) = (pe^t + 1 - p)^n$ at $t = 0$, we get $E(X) = np$.\n",
    "To find $V(X)$, we use $$\\frac{d^2}{dt^2}M_x(t) = (n^2-n)(pe^t)^2(pe^t-1+p)^{n-2}+npe^t(pe^t+1-p)^{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluated at t = 0, this leads us to the result that $V(X) = np(1-p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Results\n",
    "\n",
    "1) Let $X$ and $Y$ be random variables with mgfs $M_X$ and $M_Y$. $X$ and $Y$ are said to be identically distributed if and only if $M_X(t) = M_Y(t)$ for all $t$ in som interval containing 0. \n",
    "\n",
    "2) MGF of linear transformation of random variable: If $a$ and $b$ are constants, then \n",
    "\n",
    "$$\n",
    "M_{aX+b}(t) = e^{bt}M_X(at)\n",
    "$$\n",
    "\n",
    "3) MGF of sum of independent random variables: If $X$ and $Y$ are independent random variables with mgfs $M_X$ and $M_Y$, then\n",
    "\n",
    "$$\n",
    "M_{X+Y}(t)=M_X(t) \\cdot M_Y(t)\n",
    "$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 6 \n",
    "\n",
    "Let $X \\sim \\textsf{Exp}(\\lambda)$. Find the distribution of $Y=3X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recycling the result from example 4, we have $M_x(3t) = -\\frac{\\lambda}{3t-\\lambda}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 7 \n",
    "\n",
    "Suppose $X_1, X_2, ..., X_n$ are independent identically distributed $\\textsf{Norm}(\\mu,\\sigma)$. Find the distribution of $S=X_1+X_2+...+X_n$ and $\\bar{X} = \\frac{X_1+X_2+...+X_n}{n}$. Note that the mgf of a normally distributed random variable is $M_X(t)=e^{\\mu t+\\sigma^2 t^2/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution: $M_s(t) = e^{n\\mu t + \\sigma^2t^2/2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: $M_\\bar x(t) = e^{\\mu t + \\sigma^2t^2/2n}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
